# ğŸ¤– Simple Q&A Chatbot with Ollama (Streamlit + LangChain)

## ğŸ“Œ Overview

This project is a simple Question & Answer chatbot built using:

- **Streamlit** (Frontend UI)
- **LangChain** (LLM orchestration)
- **Ollama** (Local LLMs)

The chatbot allows users to ask questions and get responses from locally running Large Language Models.

---

## ğŸš€ Features

- Local LLM integration using Ollama
- Clean Streamlit UI
- Model selection from sidebar
- Adjustable temperature
- LangChain prompt template usage
- Output parsing with StrOutputParser

---

## ğŸ›  Tech Stack

- Python  
- Streamlit  
- LangChain  
- Ollama  

---

## ğŸ“‚ Project Structure
```
1-Ollama-Chatbot/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Install Ollama

Download and install Ollama from:
https://ollama.com

Start Ollama: ``` Ollama serve ```

---

### 2ï¸âƒ£ Pull Required Models

Example:
```
ollama run llama3.1
ollama run gemma2
ollama run phi3
```
---
## Check installed models:
``` ollama list ```

---
### 4ï¸âƒ£ Create Virtual Environment
```
python -m venv venv
venv\Scripts\activate
```
----
### 5ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```
----
### 6ï¸âƒ£ Run the Application
```
python -m streamlit run app.py
```
----

---

## ğŸ¯ How It Works

1. User enters a question  
2. Prompt template formats the message  
3. Selected Ollama model generates response  
4. LangChain parses output  
5. Streamlit displays result  

---

## ğŸ“Œ Learning Objective

This project demonstrates:

- Integrating local LLMs using Ollama
- Building LLM apps with Streamlit
- Using LangChain prompt templates
- Creating interactive AI applications

---

## ğŸ‘©â€ğŸ’» Author

Built as part of hands-on learning in Generative AI.

---

â­ Feel free to fork and experiment!
