# ğŸš€ LangChain FastAPI LLM Server

A production-style LLM API server built using **FastAPI**, **LangChain LCEL**, **Groq (Llama 3.1)**, and **LangServe**.  
This project exposes a translation chain as REST API endpoints.

---

## ğŸ“Œ Project Overview

This project demonstrates how to:

- Build LLM pipelines using **LangChain LCEL**
- Integrate **Groqâ€™s Llama 3.1 model**
- Expose chains as APIs using **LangServe**
- Deploy using **FastAPI**
- Secure API keys using `.env`

The server provides automatic REST endpoints for invoking the LLM translation chain.

---

## ğŸ—ï¸ Architecture

```
Client Request
      â†“
FastAPI Server
      â†“
LangChain Prompt Template
      â†“
Groq Llama 3.1 Model
      â†“
Output Parser
      â†“
JSON Response
```

---

## ğŸ§  Tech Stack

- Python  
- FastAPI  
- LangChain (LCEL)  
- LangServe  
- Groq (Llama-3.1-8b-instant)  
- Uvicorn  
- Python-dotenv  

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ langchain-fastapi-llm-server
 â”£ ğŸ“œ main.py
 â”£ ğŸ“œ requirements.txt
 â”£ ğŸ“œ README.md
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/langchain-fastapi-llm-server.git
cd langchain-fastapi-llm-server
```

---

### 2ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
python -m venv venv
```

Activate environment:

**Windows:**
```bash
venv\Scripts\activate
```

**Mac/Linux:**
```bash
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Create `.env` File

Create a `.env` file in the root directory:

```
GROQ_API_KEY=your_actual_groq_api_key_here
```



---

##  Running the Server

```bash
uvicorn main:app --reload
```

Server runs at:

```
http://127.0.0.1:8000
```

---

##  API Documentation (Swagger UI)

Open in your browser:

```
http://127.0.0.1:8000/docs
```

---

##  Available Endpoints (Auto-generated by LangServe)

Because we used:

```python
add_routes(app, chain, path="/chain")
```

LangServe automatically provides:

| Endpoint | Description |
|-----------|------------|
| `/chain/invoke` | Invoke single request |
| `/chain/batch` | Batch processing |
| `/chain/stream` | Stream output |
| `/chain/playground` | Interactive testing UI |

---

## ğŸ§ª Example Request

### POST Request to `/chain/invoke`

```json
{
  "input": {
    "language": "French",
    "text": "Hello, how are you?"
  }
}
```

### Example Response

```json
{
  "output": "Bonjour, comment Ã§a va ?"
}
```

---

## ğŸ’¡ Core Implementation

### Prompt Template

```python
system_temp = "Translate the following into {language}"
```

### LCEL Chain

```python
chain = prompt_template | model | parser
```

This uses LangChain Expression Language (LCEL) to build modular LLM pipelines.

---

## ğŸ¯ Features

- Modular LLM pipeline using LCEL  
- Clean API exposure using FastAPI  
- Secure API key handling  
- Automatic interactive playground  
- Production-ready structure  

---

## ğŸ“ˆ Future Improvements

- Add request validation using Pydantic  
- Add logging and error handling  
- Add Docker support  
- Deploy to Render / Railway / AWS  
- Add authentication (API key protection)  

---

---

## ğŸ‘©â€ğŸ’» Author

**Pratiksha Deshmukh**  
Aspiring AI/ML & Software Engineer  

