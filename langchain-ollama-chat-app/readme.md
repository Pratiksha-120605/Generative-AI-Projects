# ğŸ§  GenAI Chat Application using Ollama & LangChain

This project is a simple Generative AI chat application built using **LangChain**, **Ollama (local LLMs)**, and **Streamlit**.  
It allows users to interact with a locally hosted LLM without relying on paid cloud APIs.

---

## ğŸš€ Features
- Local LLM inference using Ollama
- LangChain prompt chaining
- Streamlit-based chat interface
- No OpenAI / paid API dependency
- Fast and privacy-friendly

---

## ğŸ›  Tech Stack
- Python
- LangChain
- Ollama (LLaMA / Gemma models)
- Streamlit

---

## ğŸ§© Architecture
User Input â†’ Prompt Template â†’ Ollama LLM â†’ Response â†’ UI

---

## âš™ï¸ Setup & Run

### 1. Install dependencies
```bash
pip install -r requirements.txt
```
### 2. Pull model
```bash
streamlit run app.py
```
### 3. Run the app
```bash
streamlit run app.py

```
## Demo
<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/77968681-1c65-4047-b82f-8b411450789d" />

## ğŸ“Œ Future Enhancements

Chat memory

RAG with FAISS

Multiple model selection

Streaming responses
